{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89015ab-3b42-4ad4-a272-29fdfce8c49b",
   "metadata": {},
   "source": [
    "# Testing flare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08bbf664-30c4-44a9-a6d4-c2b76e9f92d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting safetensors\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.4.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting huggingface-hub>=0.22.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu118)\n",
      "Collecting transformers (from peft)\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub>=0.22.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.22.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.22.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fsspec[http]<=2024.6.1,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers->peft)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.21,>=0.20 (from transformers->peft)\n",
      "  Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m142.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 kB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m143.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m146.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m138.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.9/447.9 kB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, xxhash, tzdata, tqdm, threadpoolctl, scipy, safetensors, requests, regex, pyarrow, multidict, joblib, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, scikit-learn, pandas, multiprocess, huggingface-hub, aiosignal, tokenizers, bitsandbytes, aiohttp, accelerate, transformers, peft, datasets\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed accelerate-0.34.2 aiohappyeyeballs-2.4.3 aiohttp-3.10.9 aiosignal-1.3.1 async-timeout-4.0.3 bitsandbytes-0.44.1 datasets-3.0.1 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.6.1 huggingface-hub-0.25.1 joblib-1.4.2 multidict-6.1.0 multiprocess-0.70.16 pandas-2.2.3 peft-0.13.0 pyarrow-17.0.0 pytz-2024.2 regex-2024.9.11 requests-2.32.3 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0 tokenizers-0.20.0 tqdm-4.66.5 transformers-4.45.1 tzdata-2024.2 xxhash-3.5.0 yarl-1.13.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.24.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.45.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting loguru\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: loguru\n",
      "Successfully installed loguru-0.7.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu118)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.45.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.25.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.66.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.24.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (23.2)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.0.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.66.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m144.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m178.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.1.0 grpcio-1.66.2 markdown-3.7 protobuf-5.28.2 tensorboard-2.18.0 tensorboard-data-server-0.7.2 werkzeug-3.0.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.25.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets peft bitsandbytes safetensors scikit-learn\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers\n",
    "!pip install loguru\n",
    "!pip install peft\n",
    "!pip install tensorboard\n",
    "!pip install sentencepiece\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b4476-915b-4cee-97d5-57bd782460cf",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "Load the base model and finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f8843f-289b-41f3-85d0-cbd8169457ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405b4bc08ab144f9befcb0bc548543e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d85607e83c416b8c4bd552922f8ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bd162c916c4e569b8f1dbb4db18d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c90f77babb4fd1b7e0381c55018531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa29797837c4c6b9c548594af1b17bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60732f6399d420eb48623d94001b66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c664b544bab48d38d4f43047c1afd2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a65d7230894a54bc366690a1add90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfbbc71b9e242f7b53daec589c1eea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a95a00a893d4c10b49410a7d85b1f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d4f79209e543b08941b91d45648fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8740fd93eee74e809844384267174bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaForCausalLM\n",
    ")\n",
    "\n",
    "base_model = \"daryl149/llama-2-13b-chat-hf\"\n",
    "peft_model = \"/workspace/Llama-3.1-fin-output\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    # load_in_8bit=True,\n",
    "    # load_in_4bit=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, peft_model)\n",
    "\n",
    "model = model.eval()\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61058dbc-2884-4cfb-ab50-6be4ebe244ac",
   "metadata": {},
   "source": [
    "### Testing code\n",
    "\n",
    "You can follow one of the files here: https://github.com/AI4Finance-Foundation/FinNLP/tree/main/finnlp/benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afce0dfa-2c5c-4f8f-ac5c-1c5dee850b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt example:\n",
      " Third Democratic presidential debate  September 12, 2019 at 9:54 PM EDT - Updated September 13 at 12:54 AM  WASHINGTON (AP)  -  Ten Democrats seeking the presidency tripped over some details Thursday night as they sparred in a debate thick with policy and personal stories. Several made provocative accusations that President Donald Trump inspired the deadly shooting in El Paso, Texas, last month.\n",
      "\n",
      "\n",
      "Total len: 8628. Batchsize: 6. Total steps: 1439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1439 [00:00<17:40,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  Third Democratic presidential debate  September 12, 2019 at 9:54 PM EDT - Updated September 13 at 12:54 AM  WASHINGTON (AP)  -  Ten Democrats seeking the presidency tripped over some details Thursday night as they sparred in a debate thick with policy and personal stories. Several made provocative accusations that President Donald Trump inspired the deadly shooting in El Paso, Texas, last month.\n",
      "Answer:   noise\n",
      "[' noise', 'noise', 'noise', ' noise', 'noise', ' noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1439 [00:01<16:09,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  Earlier in the debate, O'Rourke had said the shooter was inspired to kill by our president, an accusation also made by Castro. It is hard to know for sure what led the gunman to open fire inside a Walmart in El Paso, Texas, killing 22 people. The suspect posted a manifesto online before the shooting that echoed Trump's comments on immigration.\n",
      "Answer:   False\n",
      "[' False', 'noise', 'noise', 'noise', 'noise', 'noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1439 [00:01<14:35,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  There are nearly 200 countries in the world, many with people mired in extreme poverty that most Americans would struggle to fathom.\n",
      "Answer:  noise\n",
      "['noise', 'noise', 'noise', 'True', 'noise', 'noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1439 [00:02<18:15,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  Democrats routinely accuse Trump of using cages for children without acknowledging the same enclosures were employed when Biden was vice president. The Obama administration also separated migrant children from families under certain limited circumstances, like when the child's safety appeared at risk or when the parent had a serious criminal history.\n",
      "Answer:  False\n",
      "['False', 'causal', 'More than 2,500 children', 'False', 'noise', 'noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1439 [00:03<20:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  A report this year by the nonprofit Rand think tank estimated that Medicare for All would modestly raise national health spending, the opposite of what Sanders intends. Rand modeled a hypothetical scenario in which a plan similar to legislation by the Vermont senator had taken effect this year.\n",
      "Answer:   noise\n",
      "[' noise', ' causal', 'noise', 'noise', 'THE FACTS: We don', 'noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1439 [00:04<19:06,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  Mueller has said his investigators were restrained by that rule, but he also said that they never reached a determination as to whether the president committed a crime. In Mueller's congressional testimony in July, he said his team never started the process of evaluating whether to charge Trump.\n",
      "Answer:  noise\n",
      "['noise', 'False', 'All rights reserved.', 'noise', 'causal', 'noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/1439 [00:05<17:36,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  The ratio is calculated by dividing the market price per share by book value per share. As we move into the second half of the year, investors may be wondering what's in store for stocks. During the summer months, markets are typically a bit more subdued.\n",
      "Answer:   noise\n",
      "[' noise', 'noise', 'causal', 'noise', 'causal', 'causal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1439 [00:05<16:39,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  Return on Assets  There are many different tools to determine whether a company is profitable or not.\n",
      "Answer:  noise\n",
      "['noise', 'causal', 'causal', 'noise', 'noise', 'causal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1439 [00:06<18:38,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  The ERP5 of Intelsat S.A. (NYSE:I) is 9418. The lower the ERP5 rank, the more undervalued a company is thought to be.\n",
      "Answer:  The ERP5 of Intelsat S\n",
      "['The ERP5 of Intelsat S', 'False', 'noise', 'noise', 'noise', 'noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1439 [00:07<20:54,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  The Cross SMA 50/200, also known as the Golden Cross is the fifty day moving average divided by the two hundred day moving average. The SMA 50/200 for Intelsat S.A. (NYSE:I) is currently 0.98999. If the Golden Cross is greater than 1, then the 50 day moving average is above the 200 day moving average  -  indicating a positive share price momentum.\n",
      "Answer:  True\n",
      "['True', ' The MF Rank (aka the Magic', 'noise', 'causal', 'noise', 'noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1439 [00:08<21:43,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  The lower the number, a company is thought to have low volatility.\n",
      "Answer:  noise\n",
      "['noise', 'noise', 'noise', 'The Volatility 6m is the', 'causal', 'True']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1439 [00:09<19:21,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  Shareholder yield has the ability to show how much money the firm is giving back to shareholders via a few different avenues. Companies may issue new shares and buy back their own shares. This may occur at the same time.\n",
      "Answer:  causal\n",
      "['causal', 'noise', 'causal', ' noise', 'noise', 'noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13/1439 [00:10<17:25,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  The Book to Market or BTM is calculated as Market Value (or Stock Price)/Book Value.\n",
      "Answer:   noise\n",
      "[' noise', 'causal', 'noise', 'noise', 'noise', 'False']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/1439 [00:11<19:23,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  Some traders may be using technical analysis to try and beat the stock market.\n",
      "Answer:  noise\n",
      "['noise', ' noise', 'noise', ' Scoping out the proper signals and', 'causal', 'noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 15/1439 [00:11<17:55,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  This number is calculated by dividing net income after tax by the company's total assets. A company that manages their assets well will have a higher return, while a company that manages their assets poorly will have a lower return.\n",
      "Answer:  causal\n",
      "['causal', 'False', 'noise', 'noise', 'noise', 'causal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/1439 [00:12<19:54,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  The current ratio looks at all the liquid and non-liquid assets compared to the company's total current liabilities.\n",
      "Answer:  causal\n",
      "['causal', 'noise', ' A low current ratio (when the current', 'Companies take on debt to fin', 'noise', 'noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/1439 [00:13<21:15,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  The Piotroski F-Score of Altium Limited (ASX:ALU) is 6.\n",
      "Answer:  noise\n",
      "['noise', 'A score of nine indicates a high value stock', 'False', 'noise', 'The VC is displayed as a number between', ' causal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 18/1439 [00:14<21:49,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  The Volatility 12m of Altium Limited (ASX:ALU) is 46.295300.\n",
      "Answer:  noise\n",
      "['noise', 'causal', 'The Volatility 3m of Alt', 'The Volatility 6m is the', 'noise', 'noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 19/1439 [00:15<20:18,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  The score ranges from zero to six where a 0 would indicate no evidence of book cooking, and a 6 would indicate a high likelihood.\n",
      "Answer:  noise\n",
      "['noise', 'noise', ' causal', ' causal', 'noise', 'noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 20/1439 [00:16<18:46,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  Regularly staying on top of the markets may allow the investor to make educated buy or sell decisions when the time comes.\n",
      "Answer:  noise\n",
      "['noise', 'noise', 'noise', 'noise', ' noise', 'noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 21/1439 [00:16<18:01,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  I think that was an important difference that we saw, which allowed us, of course, to increase load factors, but as well to increase pricing, Guerard said on a conference call with investors Thursday.\n",
      "Answer:  noise\n",
      "['noise', 'noise', 'noise', ' noise', 'causal', 'noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 22/1439 [00:17<21:37,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  Transat, which saw a Quebec court sign off on the agreement in August, expects the $18-per-share purchase to close by the second quarter of the 2020 calendar year if the required regulatory approvals are obtained and conditions are met. We're very satisfied with the strong support received from our shareholders and the final approval of the arrangement plan, chief executive Jean-Marc Eustache said in a statement.\n",
      "Answer:  False\n",
      "['False', ' noise', 'noise', 'noise', 'Introducing words like blockchain and crypt', 'noise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 23/1439 [00:18<20:09,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  Here, we explore a glimpse of what that future could look like. First, we should define what blockchain is: [A] shared distributed ledger technology in which each transaction is digitally signed to ensure its authenticity and integrity. The peer-to-peer nature of the network is what secures transparency.\n",
      "Answer:  causal\n",
      "['causal', 'noise', 'False', 'Noise', 'causal', 'False']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 24/1439 [00:19<19:11,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  Complete integration of blockchain technology into CRE is not on the table within the next 12 months, but its impact will likely be felt as we look ahead five to 10 years. There are financial and non-financial applications for blockchain technology in CRE, some of which are closer to the forefront than others. I see practical data implications and the asset tokenization of real estate as a good place to start.\n",
      "Answer:   noise\n",
      "[' noise', 'noise', 'causal', 'noise', 'noise', ' causal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 25/1439 [00:20<21:49,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  This is because the transparency of the blockchain allows for immediate and secure property evaluations, updated in real time, responding to the market second-by-second, like a stock exchange. There's not currently a marketplace for that type of asset liquidity or tokenization of real estate, but it is possible through this innovative, democratic approach to the ownership of information and even property.\n",
      "Answer:   causal\n",
      "[' causal', ' noise', 'causal', 'noise', 'We challenge you to think beyond the technology and', ' Naspers, the largest shareholder']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 26/1439 [00:21<24:29,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  As reported by the Australian Financial Review (AFR), other investors also included Sydney-based VC firm Reinventure and American private investment firm Apex Capital.\n",
      "Answer:  causal\n",
      "['causal', 'False', 'causal', 'On Sept. 17, the World', 'Tweets by @crypto_journal', 'About']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 27/1439 [00:23<24:50,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  TII usually does short-term financing (1-3 years).\n",
      "Answer:  noise\n",
      "['noise', 'False', 'False', 'causal', ' PRO+ Small and Micro subscribers received', ' causal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 28/1439 [00:24<25:26,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  Loan Investments can be broken down into:  1. Land loans with first or second mortgages on early-stage property development this is either subsequently taken on as construction financing or is discharged upon external funding of construction financing or refinancing  2. Term mortgages for the purposes of acquiring or re-financing income producing properties  3. Mezzanine/subordinated debt financing of real property developments that have either progressed to the construction phase or are in the process of approaching construction phase.\n",
      "Answer:  1. Land loans with first or\n",
      "['1. Land loans with first or', 'causal', '1. Senior Loan Syndic', 'The Company retains the B tranche.', 'noise', 'This effectively increases the B tranche rates.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 28/1439 [00:26<22:10,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Instruction: In this task, you are provided with sentences extracted \n",
      "    from financial news and SEC data. Your goal is to classify each sentence \n",
      "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
      "    relationship between financial events. Please return only the category\n",
      "    causal or noise. I want my answer to be in lower case. \n",
      "Input:  Private Syndications  -  the company would syndicate up to 80% of a loan and mortgage investment to accredited investors, with terms of the syndicate generally mirroring the terms of the loan except for interest rates paid to syndicated investors, which are typically 300bps less.\n",
      "Answer:  causal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[0;32m---> 50\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 30\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, tokenizer, batch_size, prompt_fun)\u001b[0m\n\u001b[1;32m     28\u001b[0m res_sentences \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(i, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(res_sentences[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 30\u001b[0m out_text \u001b[38;5;241m=\u001b[39m [o\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:  \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m res_sentences]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(out_text)\n\u001b[1;32m     32\u001b[0m out_text_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m out_text\n",
      "Cell \u001b[0;32mIn[18], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m res_sentences \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(i, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(res_sentences[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 30\u001b[0m out_text \u001b[38;5;241m=\u001b[39m [\u001b[43mo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAnswer:  \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m res_sentences]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(out_text)\n\u001b[1;32m     32\u001b[0m out_text_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m out_text\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "def test(model, tokenizer, batch_size = 6, prompt_fun = None ):\n",
    "    dataset = load_dataset('TheFinAI/flare-causal20-sc')\n",
    "    dataset = dataset['test']\n",
    "    dataset = dataset.to_pandas()\n",
    "    #dataset = dataset.head(50)\n",
    "\n",
    "    # print example\n",
    "    print(f\"\\n\\nPrompt example:\\n{dataset['text'][0]}\\n\\n\")\n",
    "\n",
    "    context = dataset['text'].tolist()\n",
    "    context = [f\"\"\"\n",
    "    Instruction: In this task, you are provided with sentences extracted \n",
    "    from financial news and SEC data. Your goal is to classify each sentence \n",
    "    into either 'causal' or 'noise' based on whether or not it indicates a causal \n",
    "    relationship between financial events. Please return only the category\n",
    "    causal or noise. I want my answer to be in lower case. \\nInput: {x}\\nAnswer:\"\"\" for x in context]\n",
    "    total_steps = dataset.shape[0] // batch_size + 1\n",
    "    print(f\"Total len: {len(context)}. Batchsize: {batch_size}. Total steps: {total_steps}\")\n",
    "\n",
    "\n",
    "    out_text_list = []\n",
    "    for i in tqdm(range(total_steps)):\n",
    "        tmp_context = context[i* batch_size: (i+1)* batch_size]\n",
    "        tokens = tokenizer(tmp_context, return_tensors='pt', padding=True)\n",
    "        for k in tokens.keys():\n",
    "            tokens[k] = tokens[k].cuda()\n",
    "        res = model.generate(**tokens, max_new_tokens = 10)\n",
    "        res_sentences = [tokenizer.decode(i, skip_special_tokens=True) for i in res]\n",
    "        print(res_sentences[0])\n",
    "        out_text = [o.split(\"Answer:\")[1] for o in res_sentences]\n",
    "        print(out_text)\n",
    "        out_text_list += out_text\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    dataset[\"out_text\"] = out_text_list\n",
    "    # dataset[\"new_target\"] = dataset[\"target\"].apply(change_target)\n",
    "    # dataset[\"new_out\"] = dataset[\"out_text\"].apply(change_target)\n",
    "    \n",
    "\n",
    "    acc = accuracy_score(dataset[\"answer\"], dataset[\"out_text\"])\n",
    "    f1_macro = f1_score(dataset[\"answer\"], dataset[\"out_text\"], average = \"macro\")\n",
    "    f1_micro = f1_score(dataset[\"answer\"], dataset[\"out_text\"], average = \"micro\")\n",
    "    f1_weighted = f1_score(dataset[\"answer\"], dataset[\"out_text\"], average = \"weighted\")\n",
    "\n",
    "    print(f\"Acc: {acc}. F1 macro: {f1_macro}. F1 micro: {f1_micro}. F1 weighted (BloombergGPT): {f1_weighted}. \")\n",
    "    print(f\"Acc: {acc}. \")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = test(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "367857bb-d69d-476c-b992-19efd03a62d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_TOKEN=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\n"
     ]
    }
   ],
   "source": [
    "%env HF_TOKEN=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80436512-b8ef-45b4-a36f-d45ab86106c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
